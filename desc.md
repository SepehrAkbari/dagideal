### Abstract

Higher-order cumulant tensors are fundamental to non-Gaussian latent variable models and causal representation learning. Classical Independent Component Analysis (ICA) assumes mutually independent sources, corresponding algebraically to a generic or strictly diagonal cumulant tensor. This project relaxes this assumption by introducing a combinatorial graph structure $G$ that dictates allowable dependencies among latent sources, forcing specific coordinates of the mixing tensor to zero. We investigate the algebraic geometry of these graph-constrained models by studying the secant varieties of linear sections of the Segre embedding, denoted $\sigma_r(\text{Seg}(\mathbb{P}V_1 \times \cdots \times \mathbb{P}V_k) \cap L_G)$. By formulating the constrained CANDECOMP/PARAFAC (CP) decomposition as a parameterized polynomial system, we utilize Gröbner bases and tensor flattenings to compute the defining ideals of these restricted secant varieties for specific families of small graphs. This computational framework provides explicit algebraic boundaries and identifiability certificates for structured causal models, bridging classical tensor geometry with modern machine learning identifiability theory.

---


### The Secant Varieties of Graph-Constrained Cumulant Tensors

This project translates the combinatorial constraints of causal graphical models into the language of pure tensor geometry, utilizing computational algebra to find explicit solutions. It serves as a geometric foundation for recent advances in linear causal disentanglement.

#### The Mathematical Setting
Let $T$ be a tensor in $\otimes_{i=1}^d V_i$, representing the $d$th order observable cumulants of a statistical model.

* **The Unconstrained Space:** The standard rank-$r$ CP decomposition represents $T$ as a sum of $r$ rank-1 tensors:

$$
T = \sum_{j=1}^r \lambda_j v_{1,j} \otimes v_{2,j} \otimes \cdots \otimes v_{d,j}
$$

Geometrically, the closure of the set of such tensors is the $r$-th secant variety of the Segre embedding, $\sigma_r(\text{Seg}(\mathbb{P}V_1 \times \cdots \times \mathbb{P}V_k))$. The defining equations of this variety are classic objects of study in Landsberg's work (e.g., Strassen's equations, minors of flattenings).

* **The Constrained Space:** We introduce a Directed Acyclic Graph (DAG) $G = (V,E)$ representing causal dependencies. This graph dictates that certain pathways between variables do not exist. Algebraically, this forces a specific subset of the coordinates in the component vectors $v_{i,j}$ to be strictly zero.
* **The Geometric Object:** This constraint restricts our component vectors to a linear subspace $L_G$. The core mathematical objective is to find the defining ideal of the restricted secant variety $\sigma_r(\text{Seg} \cap L_G)$.

#### The Computational Pipeline

Finding the defining equations of $\sigma_r(\text{Seg} \cap L_G)$ analytically is highly non-trivial. We will attack this using implicitization via Gröbner bases.

* **Polynomial Parametrization:** We treat the non-zero entries of the component vectors $v_{i,j}$ as formal polynomial variables. The entries of the tensor $T$ (denoted $T_{i_1,\dots,i_d}$) become polynomials in these variables.
* **The Ideal Formulation:** We construct the ideal $I$ generated by the relations:

$$
t_{i_1,\dots,i_d} - f_{i_1,\dots,i_d}(v)
$$
where $f_{i_1,\dots,i_d}$ are the polynomials dictated by the constrained CP decomposition.
* **Implicitization:** Using elimination theory, we compute the Gröbner basis of $I$ with respect to a lexicographic ordering that eliminates the parameter variables $v$. The remaining polynomials in the variables $t_{i_1,\dots,i_d}$ are the implicit equations defining the algebraic boundary of the causal model.

#### Execution Strategy & Tooling

Because the complexity of Gröbner basis computation is doubly exponential in the number of variables, the strategy requires careful algorithmic orchestration:

1. **Python & NumPy Infrastructure:** You will build a Python environment to generate the symbolic matrices. You can use NumPy arrays populated with symbolic variables (via SymPy) to handle the complex index notation, tensor products, and generation of matrix flattenings.
2. **Computer Algebra Bridge:** Python will format the polynomial rings and ideals and pass them to a dedicated computer algebra system optimized for Gröbner bases (like Macaulay2 or Singular).
3. **Exploiting Flattenings:** To speed up computation, you won't just blindly compute the Gröbner basis of the full parameter space. You will first compute the $(r + 1)\times (r + 1)$ minors of the principal flattenings of the constrained tensor. These minors belong to the defining ideal and can be used to drastically reduce the search space for the full Gröbner basis.

#### Specific Targets & Deliverables

To ensure the paper is concise and publishable, the project will focus on 2 to 3 specific, small graphs highly relevant to machine learning:

* **Target 1: The Confounding Triangle.** A 3-node graph with one unobserved confounder. We will find the exact geometric boundary that distinguishes this from a purely independent ICA model.
* **Target 2: The Bipartite Structure.** A simplified structure representing standard attention-masking or sparse bipartite matching in latent layers.

**Deliverable:** A publish-ready paper detailing the geometric setup, presenting the defining ideals for these specific graph families, and providing the open-source Python/Macaulay2 codebase used to generate the certificates.